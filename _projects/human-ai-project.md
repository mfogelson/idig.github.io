---
title: Human & AI Hybrid Teams
description: |
  Assessing the Impact of a Deep Learning AI on Distributed Human Design Teams

people:
  - SeanR
  - Guanglu

layout: project
image: /img/ImpactOfAIOnTeams1.jpeg
last-updated: 2022-12-19
---

Although teaming is a prevalent approach to engineering design, human-AI interaction is under-studied in this context. This research assesses the impact of a deep learning AI on the performance, behavior, and perceived workload of distributed design teams through a human subject study that includes an abrupt problem change.

In the human subject study, participants in teams of three are asked to design truss bridges that satisfy specified design requirements using a GUI. Participants can load their teammates’ designs through the GUI anytime in the study. Among the 24 human design teams in the study, half of the design teams are human-only teams with no available deep learning AI. The other half of the design teams, designated as hybrid teams, have a deep learning AI to advise them for design modifications to the evolving truss bridge. Suggestions generated by the deep learning AI are provided as a heatmap. Participants in the hybrid teams must interpret the heatmap suggestions based on intensity of color.


The results of this study demonstrate that the AI boosts the initial performance of low-performing teams in the first session but by the end of the first session there is parity. Once the problem changes, the AI does not help or hurt the performance of low-performing teams. In contrast, the AI hurts the performance of high-performing teams in both sessions. In the post-experiment questionnaire, participants in the high-performing hybrid teams believe they accomplish the design task more successfully compared to participants in the high-performing human-only teams, but in fact their performance is worse. In other words, participants in the high-performing hybrid teams have an illusion of success. Participants in the high-performing hybrid teams also perceive less mental demand compared to participants in the high-performing human-only teams. In addition, most of the participants in the hybrid teams have either high or low confidence in the AI, with few having moderate confidence. In contrast, almost all participants have neither extremely high nor extremely low self-confidence.


The reduced performance of high-performing teams is explained through the cognitive overload, the flawed inference from AI suggestions, and a lack of motivation among participants to find better designs in the study. These potential reasons provide insights for researchers to develop AI agents that provide suggestions that are straightforward for human designers to follow or at least easy to interpret, and not overwhelm the user with too many options to consider at a time. The suggestions from AI also should eventually guide human designers towards improved designs.

Although AI has great potential to partner with human problem solvers, this research shows that it does not always improve performance in engineering design and may result in an illusion of success or reduce human effort in solving a problem. The context and interaction of the AI is critical for effectiveness and must be a core area of focus in the design of effective collaborative AIs.



Evolution and impact of human confidence on adoption of AI advice

Although artificial intelligence (AI) has shown its promise in assisting human decision-making, humans’ inappropriate decision to accept or reject AI suggestions often leads to severe consequences in high-stakes AI-assisted decision-making scenarios. This work studies how the two major components of human trust, their confidence in AI and confidence in themselves, evolve and affect humans’ decisions.

A human subject study and a quantitative model are used to understand human cognition and AI acceptance behavior during an AI-assisted decision-making scenario. The cognitive study is designed to reveal the real-time variation in human confidence in AI and human self-confidence during AI-assisted decision-making as a result of AI performance change. The quantitative model of human confidence is developed to capture the impact of different experiences during AI-assisted decision-making on confidence dynamics.


Results of this work reveal a human tendency to misattribute the blame for poor AI performance to themselves, a significant impact of human self-confidence on their decisions to accept or reject AI suggestions, and a resulting vicious cycle that hinders effective human-AI decision-making. This research suggests that poor AI performance decreases human self-confidence which is found to drive the decision to accept or reject AI suggestions. This misattribution exposes many human decision-makers to a vicious cycle of relying on a poorly performing AI. Although good decision-makers can break out of this cycle, many others cannot as their decreased self-confidence from the misattribution inclines them to accept the next suggestion from a poorly performing AI.


Our results directly affect the success of AI-assisted decision-making by providing insight into the cause of human mis-reliance on AI. These results also inspire new strategies for confidence calibration to reduce such mis-reliance. Finally, they shine light on the significance of human self-confidence in AI-assisted decision-making.